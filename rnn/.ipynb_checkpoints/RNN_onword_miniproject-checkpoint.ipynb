{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. He lay on his armour-like back, and if he lifted his head a little he could see his brown belly, slightly domed and divided by arches into stiff sections. The bedding was hardly able to cover it and seemed ready to slide off any moment. His many legs, pitifully thin compared with the size of the rest of him, waved about helplessly as he looked.\n",
      "\n",
      "\"What's happened to me?\" he thought. It wasn't a dream. His room, a proper human room although a little too small, lay peacefully between its four familiar walls. A collection of textile samples lay spread out on the table - Samsa was a travelling salesman - and above it there hung a picture that he had recently cut out of an illustrated magazine and housed in a nice, gilded frame. It showed a lady fitted out with a fur hat and fur boa who sat upright, raising a heavy fur muff that covered the whole of her lower arm towards \n",
      "data has 3082579 chars, 62 unique 3897 words\n"
     ]
    }
   ],
   "source": [
    "#data = open('kafka.txt', 'r').read()\n",
    "#data = open('compression_data.txt',encoding=\"utf8\").read()\n",
    "data = open('compression_data_6.txt',encoding=\"ISO-8859-1\").read()\n",
    "#data=data.replace(' ', '_')\n",
    "#data=data.replace('',' # ')\n",
    "#data.rstrip()\n",
    "print(data[:1000])\n",
    "#data=data.replace(' ', ' _ ')\n",
    "data_list=data.split(' ')\n",
    "#data=\"Neural networks operate on vectors (a vector is an array of float).\"\n",
    "chars = list(set(data))\n",
    "data_word_len=len(data_list)\n",
    "words= list(set(data_list))\n",
    "data_size, vocab_size, word_size = len(data), len(chars), len(words)\n",
    "print ('data has %d chars, %d unique %d words' % (data_size, vocab_size, word_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1904575"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data_list\n",
    "len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_to_ix={word:i for i,word in enumerate(words)}\n",
    "ix_to_word={i:word for i,word in enumerate(words)}\n",
    "print(word_to_ix)\n",
    "print('\\n')\n",
    "print(ix_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "vector_for_word_twinkle = np.zeros((word_size, 1))\n",
    "vector_for_word_twinkle[word_to_ix['a']] = 1\n",
    "print (vector_for_word_twinkle.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model parameters\n",
    "\n",
    "hidden_size = 100\n",
    "seq_length = 5\n",
    "learning_rate = 1e-1\n",
    "\n",
    "Wxh = np.random.randn(hidden_size, word_size) * 0.01 #input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01 #input to hidden\n",
    "Why = np.random.randn(word_size, hidden_size) * 0.01 #input to hidden\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((word_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    t=0\n",
    "  #\"\"\"                                                                                                                                                                                         \n",
    "  #inputs,targets are both list of integers.                                                                                                                                                   \n",
    "  #hprev is Hx1 array of initial hidden state                                                                                                                                                  \n",
    "  #returns the loss, gradients on model parameters, and last hidden state                                                                                                                      \n",
    "  #\"\"\"\n",
    "  #store our inputs, hidden states, outputs, and probability values\n",
    "    xs, hs, ys, ps, = {}, {}, {}, {} #Empty dicts\n",
    "        # Each of these are going to be SEQ_LENGTH(Here 25) long dicts i.e. 1 vector per time(seq) step\n",
    "        # xs will store 1 hot encoded input characters for each of 25 time steps (26, 25 times)\n",
    "        # hs will store hidden state outputs for 25 time steps (100, 25 times)) plus a -1 indexed initial state\n",
    "        # to calculate the hidden state at t = 0\n",
    "        # ys will store targets i.e. expected outputs for 25 times (26, 25 times), unnormalized probabs\n",
    "        # ps will take the ys and convert them to normalized probab for chars\n",
    "        # We could have used lists BUT we need an entry with -1 to calc the 0th hidden layer\n",
    "        # -1 as  a list index would wrap around to the final element\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "      #init with previous hidden state\n",
    "        # Using \"=\" would create a reference, this creates a whole separate copy\n",
    "        # We don't want hs[-1] to automatically change if hprev is changed\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    #init loss as 0\n",
    "    loss = 0\n",
    "  # forward pass                                                                                                                                                                              \n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((word_size,1)) # encode in 1-of-k representation (we place a 0 vector as the t-th input)                                                                                                                     \n",
    "        xs[t][inputs[t]] = 1 # Inside that t-th input we use the integer in \"inputs\" list to  set the correct\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state                                                                                                            \n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars                                                                                                           \n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars                                                                                                              \n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)                                                                                                                       \n",
    "  # backward pass: compute gradients going backwards    \n",
    "  #initalize vectors for gradient values for each set of weights \n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        #output probabilities\n",
    "        dy = np.copy(ps[t])\n",
    "        #derive our first gradient\n",
    "        dy[targets[t]] -= 1 # backprop into y  \n",
    "        #compute output gradient -  output times hidden states transpose\n",
    "        #When we apply the transpose weight matrix,  \n",
    "        #we can think intuitively of this as moving the error backward\n",
    "        #through the network, giving us some sort of measure of the error \n",
    "        #at the output of the lth layer. \n",
    "        #output gradient\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        #derivative of output bias\n",
    "        dby += dy\n",
    "        #backpropagate!\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h                                                                                                                                         \n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity                                                                                                                     \n",
    "        dbh += dhraw #derivative of hidden bias\n",
    "        dWxh += np.dot(dhraw, xs[t].T) #derivative of input to hidden layer weight\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T) #derivative of hidden layer to hidden layer weight\n",
    "        dhnext = np.dot(Whh.T, dhraw) \n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients                                                                                                                 \n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(uncompressed):\n",
    "    \"\"\"Compress a string to a list of output symbols.\"\"\"\n",
    " \n",
    "    # Build the dictionary.\n",
    "    dict_size = 256\n",
    "    #dictionary = dict((chr(i), i) for i in range(dict_size))\n",
    "    # in Python 3:\n",
    "    dictionary = {chr(i): i for i in range(dict_size)}\n",
    " \n",
    "    w = \"\"\n",
    "    result = []\n",
    "    for c in uncompressed:\n",
    "        wc = w + c\n",
    "        if wc in dictionary:\n",
    "            w = wc\n",
    "        else:\n",
    "            result.append(dictionary[w])\n",
    "            # Add wc to the dictionary.\n",
    "            dictionary[wc] = dict_size\n",
    "            dict_size += 1\n",
    "            w = c\n",
    " \n",
    "    # Output the code for w.\n",
    "    if w:\n",
    "        result.append(dictionary[w])\n",
    "    return result\n",
    " \n",
    " \n",
    "def decompress(compressed):\n",
    "    \"\"\"Decompress a list of output ks to a string.\"\"\"\n",
    "    try:\n",
    "        from cStringIO import StringIO\n",
    "    except:\n",
    "        from io import StringIO\n",
    "    # Build the dictionary.\n",
    "    dict_size = 256\n",
    "    #dictionary = dict((i, chr(i)) for i in xrange(dict_size))\n",
    "    # in Python 3: \n",
    "    dictionary = {i: chr(i) for i in range(dict_size)}\n",
    " \n",
    "    # use StringIO, otherwise this becomes O(N^2)\n",
    "    # due to string concatenation in a loop\n",
    "    result = StringIO()\n",
    "    w = chr(compressed.pop(0))\n",
    "    result.write(w)\n",
    "    for k in compressed:\n",
    "        if k in dictionary:\n",
    "            entry = dictionary[k]\n",
    "        elif k == dict_size:\n",
    "            entry = w + w[0]\n",
    "        else:\n",
    "            raise ValueError('Bad compressed k: %s' % k)\n",
    "        result.write(entry)\n",
    " \n",
    "        # Add w+entry[0] to the dictionary.\n",
    "        dictionary[dict_size] = w + entry[0]\n",
    "        dict_size += 1\n",
    " \n",
    "        w = entry\n",
    "    return result.getvalue()\n",
    " \n",
    " \n",
    " # How to use:\n",
    "compressed = compress(data)\n",
    "#compressed_directly=compress(data)\n",
    "#print (compressed)\n",
    "#decompressed = decompress(compressed)\n",
    "#print (decompressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "kii=' '.join((str(key) for key in compressed))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#prediction, one full forward pass\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\"                                                                                                                                                                                         \n",
    "    sample a sequence of integers from the model                                                                                                                                                \n",
    "    h is memory state, seed_ix is seed letter for first time step   \n",
    "    n is how many characters to predict\n",
    "    \"\"\"\n",
    "    #create vector\n",
    "    x = np.zeros((word_size, 1))\n",
    "    #customize it for our seed char\n",
    "    x[seed_ix] = 1\n",
    "    #list to store generated chars\n",
    "    ixes = []\n",
    "    #for as many characters as we want to generate\n",
    "    for t in range(n):\n",
    "        #a hidden state at a given time step is a function \n",
    "        #of the input at the same time step modified by a weight matrix \n",
    "        #added to the hidden state of the previous time step \n",
    "        #multiplied by its own hidden state to hidden state matrix.\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        #compute output (unnormalised)\n",
    "        y = np.dot(Why, h) + by\n",
    "        ## probabilities for next chars\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        #pick one with the highest probability \n",
    "        ix = np.random.choice(range(word_size), p=p.ravel())\n",
    "        #create a vector\n",
    "        x = np.zeros((word_size, 1))\n",
    "        #customize it for the predicted char\n",
    "        x[ix] = 1\n",
    "        #add it to the list\n",
    "        ixes.append(ix)\n",
    "\n",
    "    txt = ''.join(ix_to_word[ix] for ix in ixes)\n",
    "    print ('----\\n %s \\n----' % (txt, ))\n",
    "hprev = np.zeros((hidden_size,1)) # reset RNN memory  \n",
    "    #predict the 200 next characters given 'a'\n",
    "sample(hprev,word_to_ix['a'],len(data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs [1467, 2643, 2007, 2643, 3685]\n",
      "['One', '_', 'morning,', '_', 'when']\n",
      "targets [2643, 2007, 2643, 3685, 2643]\n",
      "['_', 'morning,', '_', 'when', '_']\n"
     ]
    }
   ],
   "source": [
    "p=0\n",
    "inputs = [word_to_ix[ch] for ch in data_list[p:p+seq_length]]\n",
    "print (\"inputs\", inputs)\n",
    "print (data_list[p:p+seq_length])\n",
    "targets = [word_to_ix[ch] for ch in data_list[p+1:p+seq_length+1]]\n",
    "print (\"targets\", targets)\n",
    "print (data_list[p+1:p+seq_length+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad                                                                                                                \n",
    "smooth_loss = -np.log(1.0/word_size)*seq_length # loss at iteration 0                                                                                                                        \n",
    "while n<=1000*100:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  # check \"How to feed the loss function to see how this part works\n",
    "    if p+seq_length+1 >= len(data_list) or n == 0:\n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory                                                                                                                                      \n",
    "        p = 0 # go from start of data                                                                                                                                                             \n",
    "    inputs = [word_to_ix[ch] for ch in data_list[p:p+seq_length]]\n",
    "    targets = [word_to_ix[ch] for ch in data_list[p+1:p+seq_length+1]]\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient                                                                                                                          \n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "  # sample from the model now and then                                                                                                                                                        \n",
    "    if n % 1000 == 0:\n",
    "        print ('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "        sample(hprev, inputs[0], len(data_list))\n",
    "\n",
    "  # perform parameter update with Adagrad                                                                                                                                                     \n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],[dWxh, dWhh, dWhy, dbh, dby],[mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
    "\n",
    "    p += seq_length # move data pointer                                                                                                                                                         \n",
    "    n += 1 # iteration counter\n",
    "#sample(hprev, inputs[0], len(data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=0\n",
    "inputs = [word_to_ix[ch] for ch in data_list[p:p+seq_length]]\n",
    "print(inputs)\n",
    "print(data_list[p:p+seq_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample(hprev, inputs[0], len(data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(index=np.arange(100), columns=np.arange(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF = pd.DataFrame() #creates a new dataframe that's empty\n",
    "newDF = newDF.append(pd.DataFrame(Wxh), ignore_index = True) # ignoring index is optional\n",
    "newDF = newDF.append(pd.DataFrame(Whh), ignore_index = True)\n",
    "newDF = newDF.append(pd.DataFrame(Why), ignore_index = True)\n",
    "newDF = newDF.append(pd.DataFrame(bh).T, ignore_index = True)\n",
    "newDF = newDF.append(pd.DataFrame(by).T, ignore_index = True)\n",
    "newDF = newDF.append(pd.DataFrame(np.array([hidden_size,word_size])), ignore_index = True)\n",
    "# try printing some data from newDF\n",
    "print (newDF.head()) #again optional "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF.to_csv('out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('out.csv')\n",
    "header=df.dtypes.index\n",
    "header=header.tolist()\n",
    "df.drop(header[0], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wxh_read=np.array(df.iloc[0:hidden_size,0:word_size])\n",
    "Whh_read=np.array(df.iloc[hidden_size:hidden_size*2,0:hidden_size])\n",
    "Why_read=np.array(df.iloc[hidden_size*2:hidden_size*2+word_size,0:hidden_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bh_read=np.array(df[hidden_size*2+word_size:hidden_size*2+word_size+1].T)\n",
    "by_read=np.array(df.iloc[hidden_size*2+word_size+1:hidden_size*2+word_size+2, 0:word_size].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('rnn_compression.txt','w')\n",
    "file.write(kii)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h, seed_ix, n):\n",
    "    \"\"\"                                                                                                                                                                                         \n",
    "    sample a sequence of integers from the model                                                                                                                                                \n",
    "    h is memory state, seed_ix is seed letter for first time step   \n",
    "    n is how many characters to predict\n",
    "    \"\"\"\n",
    "    #create vector\n",
    "    x = np.zeros((word_size, 1))\n",
    "    #customize it for our seed char\n",
    "    x[seed_ix] = 1\n",
    "    #list to store generated chars\n",
    "    ixes = []\n",
    "    #for as many characters as we want to generate\n",
    "    for t in range(n):\n",
    "        #a hidden state at a given time step is a function \n",
    "        #of the input at the same time step modified by a weight matrix \n",
    "        #added to the hidden state of the previous time step \n",
    "        #multiplied by its own hidden state to hidden state matrix.\n",
    "        h = np.tanh(np.dot(Wxh_read, x) + np.dot(Whh_read, h) + bh_read)\n",
    "        #compute output (unnormalised)\n",
    "        y = np.dot(Why_read, h) + by_read\n",
    "        ## probabilities for next chars\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        #pick one with the highest probability \n",
    "        ix = np.random.choice(range(word_size), p=p.ravel())\n",
    "        #create a vector\n",
    "        x = np.zeros((word_size, 1))\n",
    "        #customize it for the predicted char\n",
    "        x[ix] = 1\n",
    "        #add it to the list\n",
    "        ixes.append(ix)\n",
    "\n",
    "    txt = ''.join(ix_to_word[ix] for ix in ixes)\n",
    "    print ('----\\n %s \\n----' % (txt, ))\n",
    "hprev = np.zeros((hidden_size,1)) # reset RNN memory  \n",
    "    #predict the 200 next characters given 'a'\n",
    "sample(hprev, inputs[0], len(data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "decompressed = decompress(compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. He lay on his armour-like back, and if he lifted his head a little he could see his brown belly, slightly domed and divided by arches into stiff sections. The bedding was hardly able to cover it and seemed ready to slide off any moment. His many legs, pitifully thin compared with the size of the rest of him, waved about helplessly as he looked.\\n\\n\"What\\'s happened to me?\" he thought. It wasn\\'t a dream. His room, a proper human room although a little too small, lay peacefully between its four familiar walls. A collection of textile samples lay spread out on the table - Samsa was a travelling salesman - and above it there hung a picture that he had recently cut out of an illustrated magazine and housed in a nice, gilded frame. It showed a lady fitted out with a fur hat and fur boa who sat upright, raising a heavy fur muff that covered the whole of her lower arm towards the viewer.\\n\\nGregor then turned to look out the window at the dull weather. Drops of rain could be heard hitting the pane, which made him feel quite sad. \"How about if I sleep a little bit longer and forget all this nonsense\", he thought, but that was something he was unable to do because he was used to sleeping on his right, and in his present state couldn\\'t get into that position. However hard he threw himself onto his right, he always rolled back to where he was. He must have tried it a hundred times, shut his eyes so that he wouldn\\'t have to look at the floundering legs, and only stopped when he began to feel a mild, dull pain there that he had never felt before.\\n\\n\"Oh, God\", he thought, \"what a strenuous career it is that I\\'ve chosen! Travelling day in and day out. Doing business like this takes much more effort than doing your own business at home, and on top of that there\\'s the curse of travelling, worries about making train connections, bad and irregular food, contact with diffe'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decompressed[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of orignal file:  6276867\n",
      "size of compressed file:  5980735\n",
      "size of decompressed file:  6276867\n"
     ]
    }
   ],
   "source": [
    "print('size of orignal file: ',len(data))\n",
    "print('size of compressed file: ',len(kii))\n",
    "print('size of decompressed file: ',len(decompressed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of orignal file:  11675226\n",
      "size of compressed file:  11505380\n",
      "size of decompressed file:  11675226\n"
     ]
    }
   ],
   "source": [
    "print('size of orignal file: ',len(data))\n",
    "print('size of compressed file: ',len(kii))\n",
    "print('size of decompressed file: ',len(decompressed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of orignal file:  3082579\n",
      "size of compressed file:  2462001\n",
      "size of decompressed file:  3082579\n"
     ]
    }
   ],
   "source": [
    "print('size of orignal file: ',len(data))\n",
    "print('size of compressed file: ',len(kii))\n",
    "print('size of decompressed file: ',len(decompressed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
